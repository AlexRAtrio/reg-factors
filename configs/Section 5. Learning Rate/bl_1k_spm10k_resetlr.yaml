## Where the samples will be written
save_data: models/bl_1k_spm10k_resetlr
## Where the vocab(s) will be written
src_vocab: models/bl_1k_spm10k_resetlr/vocab.src

share_vocab: true

src_words_min_frequency: 2
tgt_words_min_frequency: 2

# Subword
src_subword_type: sentencepiece
src_subword_model: /home/alejandr.ramireza/workspace/data/spm/hsb-de_10.model

# Number of candidates for SentencePiece sampling
subword_nbest: 1
# Smoothing parameter for SentencePiece sampling
subword_alpha: 0.0
# Filter length
src_seq_length: 500
tgt_seq_length: 500

# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
    corpus_1:
        path_src: /home/alejandr.ramireza/workspace/data/raw/train/hsb-de-dsb/train.hsb-de.hsb
        path_tgt: /home/alejandr.ramireza/workspace/data/raw/train/hsb-de-dsb/train.hsb-de.de
        transforms: [sentencepiece]
        weight: 1
    valid:
        path_src: /home/alejandr.ramireza/workspace/data/raw/dev/devel.hsb-de.hsb
        path_tgt: /home/alejandr.ramireza/workspace/data/raw/dev/devel.hsb-de.de
        transforms: [sentencepiece]
        weight: 1

# Set number of GPUs
world_size: 2
gpu_ranks: [0,1]

# General opts
# Where to save the checkpoints
save_model: models/bl_1k_spm10k_resetlr/bl_1k_spm10k_resetlr
save_checkpoint_steps: 30000
train_steps: 600000
valid_steps: 30000
report_every: 30000
reset_optim: all
train_from: models/bl_1k_spm10k/bl_1k_spm10k_step_600000.pt

# Batching
batch_type: tokens
batch_size: 1000
valid_batch_size: 200
max_generator_batches: 2

# Optimization
optim: adam
learning_rate: 2.0
adam_beta2: 0.998
decay_method: noam
accum_count: 2
warmup_steps: 8000
label_smoothing: 0.1
max_grad_norm: 0
param_init: 0
param_init_glorot: true
normalization: tokens

# Model
encoder_type: transformer
decoder_type: transformer
position_encoding: true
layers: 6  # Same for enc/dec
heads: 8
rnn_size: 512
word_vec_size: 512
transformer_ff: 2048
dropout: 0.1

# Logs
log_file: models/bl_1k_spm10k_resetlr/train_log
log_file_level: WARNING